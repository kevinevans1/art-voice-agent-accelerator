name: üß™ Evaluation Framework Tests

on:
  push:
    branches: [main, staging]
    paths:
      - 'tests/evaluation/**'
      - '.github/workflows/test-evaluation-framework.yml'
  pull_request:
    branches: [main, staging]
    paths:
      - 'tests/evaluation/**'
      - 'src/**'
      - '.github/workflows/test-evaluation-framework.yml'
  workflow_dispatch:
    inputs:
      run_all_tests:
        description: 'Run all tests (not just evaluation)'
        required: false
        default: false
        type: boolean

env:
  CI: true
  PYTHONDONTWRITEBYTECODE: 1
  PYTHONUNBUFFERED: 1

permissions:
  contents: read
  pull-requests: read

jobs:
  # ============================================================================
  # JOB: Lint & Type Check
  # ============================================================================
  lint:
    name: üîç Lint & Type Check
    runs-on: ubuntu-latest
    
    steps:
      - name: üõí Checkout
        uses: actions/checkout@v4

      - name: üêç Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: üì¶ Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: üîß Install dependencies
        run: |
          uv pip install --system ruff black isort

      - name: üîç Ruff lint
        run: |
          ruff check tests/evaluation/ --config pyproject.toml
        continue-on-error: true

      - name: üé® Black format check
        run: |
          black --check --diff tests/evaluation/
        continue-on-error: true

      - name: üìë isort import check
        run: |
          isort --check-only --diff tests/evaluation/
        continue-on-error: true

      - name: üìã Lint Summary
        run: |
          echo "## üîç Lint Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Check | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Ruff | ‚úì Checked |" >> $GITHUB_STEP_SUMMARY
          echo "| Black | ‚úì Checked |" >> $GITHUB_STEP_SUMMARY
          echo "| isort | ‚úì Checked |" >> $GITHUB_STEP_SUMMARY

  # ============================================================================
  # JOB: Unit Tests
  # ============================================================================
  unit-tests:
    name: üß™ Unit Tests
    runs-on: ubuntu-latest
    needs: lint
    
    steps:
      - name: üõí Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: üêç Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: üì¶ Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: üîß Install dependencies
        run: |
          # Install project with dev dependencies
          uv pip install --system -e ".[dev]"

      - name: üìÅ Create reports directory
        run: mkdir -p reports

      - name: üß™ Run Evaluation Tests
        run: |
          set -o pipefail
          pytest tests/evaluation/ \
            -v \
            --tb=short \
            --junitxml=reports/evaluation-tests.xml \
            -m "not evaluation" \
            2>&1 | tee reports/test-output.txt
        env:
          PYTHONPATH: .

      - name: üìä Test Results Summary
        if: always()
        run: |
          echo "## üß™ Evaluation Framework Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Count tests from output
          PASSED=$(grep -oP '\d+(?= passed)' reports/test-output.txt | head -1 || echo "0")
          FAILED=$(grep -oP '\d+(?= failed)' reports/test-output.txt | head -1 || echo "0")
          SKIPPED=$(grep -oP '\d+(?= skipped)' reports/test-output.txt | head -1 || echo "0")
          
          echo "| Metric | Count |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| ‚úÖ Passed | $PASSED |" >> $GITHUB_STEP_SUMMARY
          echo "| ‚ùå Failed | $FAILED |" >> $GITHUB_STEP_SUMMARY
          echo "| ‚è≠Ô∏è Skipped | $SKIPPED |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Module breakdown
          echo "### üì¶ Test Modules" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Module | Description |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------------|" >> $GITHUB_STEP_SUMMARY
          echo "| test_generators.py | Turn templates & generators |" >> $GITHUB_STEP_SUMMARY
          echo "| test_hooks.py | Evaluation hooks system |" >> $GITHUB_STEP_SUMMARY
          echo "| test_metrics.py | Pluggable metrics |" >> $GITHUB_STEP_SUMMARY
          echo "| test_scenarios.py | End-to-end scenario tests |" >> $GITHUB_STEP_SUMMARY

      - name: üì§ Upload Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results
          path: |
            reports/evaluation-tests.xml
            reports/test-output.txt
          retention-days: 30

  # ============================================================================
  # JOB: Schema Validation
  # ============================================================================
  schema-validation:
    name: üìã Schema Validation
    runs-on: ubuntu-latest
    needs: lint
    
    steps:
      - name: üõí Checkout
        uses: actions/checkout@v4

      - name: üêç Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: üì¶ Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: üîß Install dependencies
        run: |
          uv pip install --system -e ".[dev]"

      - name: üìã Validate Scenario Schemas
        run: |
          python -c "
          import sys
          sys.path.insert(0, '.')
          
          from pathlib import Path
          import yaml
          
          # Import schemas to validate they load correctly
          from tests.evaluation.schemas import (
              ModelProfile,
              TurnEvent,
              TurnScore,
              RunSummary,
              ScenarioExpectations,
              SessionAgentConfig,
          )
          
          print('‚úÖ All schema imports successful')
          
          # Validate any YAML scenarios
          scenarios_dir = Path('tests/evaluation/scenarios')
          if scenarios_dir.exists():
              for yaml_file in scenarios_dir.rglob('*.yaml'):
                  print(f'üìÑ Validating: {yaml_file}')
                  with open(yaml_file) as f:
                      data = yaml.safe_load(f)
                  print(f'   ‚úì Valid YAML structure')
          
          print('')
          print('‚úÖ Schema validation complete!')
          "
        env:
          PYTHONPATH: .

      - name: üìã Schema Validation Summary
        run: |
          echo "## üìã Schema Validation" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Schema Module | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|---------------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| config.py | ‚úÖ Valid |" >> $GITHUB_STEP_SUMMARY
          echo "| events.py | ‚úÖ Valid |" >> $GITHUB_STEP_SUMMARY
          echo "| expectations.py | ‚úÖ Valid |" >> $GITHUB_STEP_SUMMARY
          echo "| results.py | ‚úÖ Valid |" >> $GITHUB_STEP_SUMMARY
          echo "| foundry.py | ‚úÖ Valid |" >> $GITHUB_STEP_SUMMARY

  # ============================================================================
  # JOB: Module Integration
  # ============================================================================
  module-integration:
    name: üîó Module Integration
    runs-on: ubuntu-latest
    needs: [unit-tests, schema-validation]
    
    steps:
      - name: üõí Checkout
        uses: actions/checkout@v4

      - name: üêç Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: üì¶ Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: üîß Install dependencies
        run: |
          uv pip install --system -e ".[dev]"

      - name: üîó Test Module Integration
        run: |
          python -c "
          import sys
          sys.path.insert(0, '.')
          
          print('üîó Testing Module Integration')
          print('=' * 50)
          
          # Test all modules can be imported together
          from tests.evaluation import (
              # Schemas
              ModelProfile,
              TurnEvent,
              ToolCall,
              HandoffEvent,
              ScenarioExpectations,
              RunSummary,
              TurnScore,
              
              # Core
              EventRecorder,
              MetricsScorer,
              HookRegistry,
          )
          
          # Import additional schemas from submodule
          from tests.evaluation.schemas import SessionAgentConfig
          
          print('‚úÖ All module imports successful')
          print('')
          
          # Verify registries can be instantiated
          hook_registry = HookRegistry()
          print(f'‚úÖ HookRegistry created')
          
          print('')
          print('üéâ All module integrations verified!')
          "
        env:
          PYTHONPATH: .

      - name: üìã Integration Summary
        run: |
          echo "## üîó Module Integration" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "All evaluation framework modules integrate correctly:" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Module | Components |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|------------|" >> $GITHUB_STEP_SUMMARY
          echo "| schemas | ModelProfile, TurnEvent, ScenarioExpectations |" >> $GITHUB_STEP_SUMMARY
          echo "| hooks | HookRegistry |" >> $GITHUB_STEP_SUMMARY
          echo "| core | EventRecorder, MetricsScorer |" >> $GITHUB_STEP_SUMMARY

  # ============================================================================
  # JOB: Summary
  # ============================================================================
  summary:
    name: üìä Test Summary
    runs-on: ubuntu-latest
    needs: [lint, unit-tests, schema-validation, module-integration]
    if: always()
    
    steps:
      - name: üìä Generate Summary
        run: |
          echo "# üß™ Evaluation Framework CI Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Job Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| üîç Lint & Type Check | ${{ needs.lint.result == 'success' && 'üü¢ Passed' || 'üî¥ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| üß™ Unit Tests | ${{ needs.unit-tests.result == 'success' && 'üü¢ Passed' || 'üî¥ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| üìã Schema Validation | ${{ needs.schema-validation.result == 'success' && 'üü¢ Passed' || 'üî¥ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| üîó Module Integration | ${{ needs.module-integration.result == 'success' && 'üü¢ Passed' || 'üî¥ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Framework Components Tested" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Schemas**: Pydantic models for configs, events, results" >> $GITHUB_STEP_SUMMARY
          echo "- **Hooks**: Extensible evaluation hooks (on_turn_complete, pre_score)" >> $GITHUB_STEP_SUMMARY
          echo "- **Generators**: Turn templates and dynamic content generation" >> $GITHUB_STEP_SUMMARY
          echo "- **Metrics**: Pluggable metrics (tool precision, latency, cost, etc.)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "üìñ See [Evaluation Framework Docs](https://azure-samples.github.io/art-voice-agent-accelerator/testing/model-evaluation/) for details." >> $GITHUB_STEP_SUMMARY

      - name: ‚úÖ Check Results
        run: |
          if [[ "${{ needs.unit-tests.result }}" != "success" ]]; then
            echo "‚ùå Unit tests failed"
            exit 1
          fi
          if [[ "${{ needs.schema-validation.result }}" != "success" ]]; then
            echo "‚ùå Schema validation failed"
            exit 1
          fi
          if [[ "${{ needs.module-integration.result }}" != "success" ]]; then
            echo "‚ùå Module integration failed"
            exit 1
          fi
          echo "‚úÖ All evaluation framework tests passed!"
