{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Framework Playground\n",
    "\n",
    "Interactive notebook for evaluating agent performance with the cascade orchestrator.\n",
    "\n",
    "## Features\n",
    "\n",
    "- ‚úÖ Test YAML scenario loading\n",
    "- ‚úÖ Run orchestrator turns with full evaluation\n",
    "- ‚úÖ Record events and score performance\n",
    "- ‚úÖ Compare model configurations (GPT-4o vs o1, different verbosity levels)\n",
    "- ‚úÖ A/B testing capabilities\n",
    "\n",
    "## Quick Links\n",
    "\n",
    "- [Evaluation Package](../../../apps/artagent/backend/evaluation/)\n",
    "- [Documentation](../../../docs/testing/model-evals.md)\n",
    "- [Test Scenarios](../../../tests/eval_scenarios/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import asyncio\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import yaml\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Add project root to Python path\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent.parent.parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"üìÅ Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"‚úÖ Python path configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment configuration\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env first (fallback)\n",
    "env_local_path = PROJECT_ROOT / \".env.local\"\n",
    "env_path = PROJECT_ROOT / \".env\"\n",
    "\n",
    "if env_local_path.exists():\n",
    "    print(f\"‚úÖ Loading .env.local\")\n",
    "    load_dotenv(env_local_path, override=True)\n",
    "    config_source = \".env.local\"\n",
    "elif env_path.exists():\n",
    "    print(f\"‚úÖ Loading .env\")\n",
    "    load_dotenv(env_path, override=True)\n",
    "    config_source = \".env\"\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No .env file found. Using system environment variables.\")\n",
    "    config_source = \"system environment\"\n",
    "\n",
    "# Try to load Azure App Configuration (preferred)\n",
    "try:\n",
    "    from apps.artagent.backend.config.appconfig_provider import bootstrap_appconfig, get_provider_status\n",
    "    \n",
    "    appconfig_loaded = bootstrap_appconfig()\n",
    "    if appconfig_loaded:\n",
    "        status = get_provider_status()\n",
    "        endpoint_name = status.get(\"endpoint\", \"\").split(\"//\")[-1].split(\".\")[0] if status.get(\"endpoint\") else \"unknown\"\n",
    "        print(f\"‚úÖ Loaded configuration from Azure App Config ({endpoint_name})\")\n",
    "        config_source = f\"Azure App Config ({endpoint_name})\"\n",
    "except Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è  App Configuration not available, using {config_source}\")\n",
    "\n",
    "# Verify Azure OpenAI is configured\n",
    "endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "deployment = os.getenv('AZURE_OPENAI_CHAT_DEPLOYMENT_ID') or 'gpt-4o'\n",
    "\n",
    "print(f\"\\nüìã Configuration source: {config_source}\")\n",
    "if endpoint:\n",
    "    print(f\"‚úÖ Azure OpenAI endpoint: {endpoint}\")\n",
    "else:\n",
    "    print(\"‚ùå AZURE_OPENAI_ENDPOINT not set\")\n",
    "\n",
    "if deployment:\n",
    "    print(f\"‚úÖ Default deployment: {deployment}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  AZURE_OPENAI_DEPLOYMENT_NAME not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation framework\n",
    "from tests.evaluation import (\n",
    "    EventRecorder,\n",
    "    EvaluationOrchestratorWrapper,\n",
    "    MetricsScorer,\n",
    "    ComparisonRunner,\n",
    "    MockMemoManager,\n",
    "    build_context,\n",
    ")\n",
    "\n",
    "# Import orchestrator components\n",
    "from apps.artagent.backend.registries.agentstore.loader import (\n",
    "    discover_agents,\n",
    "    get_agent,\n",
    "    build_handoff_map,\n",
    ")\n",
    "from apps.artagent.backend.registries.agentstore.base import ModelConfig\n",
    "from apps.artagent.backend.voice.speech_cascade.orchestrator import (\n",
    "    CascadeOrchestratorAdapter,\n",
    ")\n",
    "from apps.artagent.backend.voice.shared.base import OrchestratorContext\n",
    "\n",
    "print(\"‚úÖ All components imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Discover Available Agents\n",
    "\n",
    "Load real agents from the agent registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover all available agents\n",
    "agents = discover_agents()\n",
    "handoff_map = build_handoff_map(agents)\n",
    "\n",
    "print(f\"üì¶ Discovered {len(agents)} agents:\\n\")\n",
    "\n",
    "# Group by category\n",
    "banking = []\n",
    "insurance = []\n",
    "other = []\n",
    "\n",
    "for name, agent in sorted(agents.items()):\n",
    "    desc = agent.description[:80] if agent.description else \"No description\"\n",
    "    \n",
    "    # Categorize based on name patterns\n",
    "    if any(x in name.lower() for x in ['banking', 'fraud', 'investment', 'card', 'auth', 'compliance']):\n",
    "        banking.append((name, desc))\n",
    "    elif any(x in name.lower() for x in ['claims', 'policy', 'fnol', 'subro']):\n",
    "        insurance.append((name, desc))\n",
    "    else:\n",
    "        other.append((name, desc))\n",
    "\n",
    "if banking:\n",
    "    print(\"Banking Agents:\")\n",
    "    for name, desc in banking:\n",
    "        print(f\"  ‚Ä¢ {name}: {desc}\")\n",
    "    print()\n",
    "\n",
    "if insurance:\n",
    "    print(\"Insurance Agents:\")\n",
    "    for name, desc in insurance:\n",
    "        print(f\"  ‚Ä¢ {name}: {desc}\")\n",
    "    print()\n",
    "\n",
    "if other:\n",
    "    print(\"Other Agents:\")\n",
    "    for name, desc in other:\n",
    "        print(f\"  ‚Ä¢ {name}: {desc}\")\n",
    "    print()\n",
    "\n",
    "print(f\"\\nüîó Handoff map: {len(handoff_map)} handoff triggers configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Custom Cascade Orchestrator instance\n",
    "\n",
    "This creates an actual orchestrator with real agents (not mocks):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_orchestrator(\n",
    "    agent_name: str,\n",
    "    model_override: dict = None,\n",
    "    session_id: str = \"eval-session\",\n",
    ") -> CascadeOrchestratorAdapter:\n",
    "    \"\"\"\n",
    "    Create a real orchestrator for evaluation.\n",
    "    \n",
    "    Args:\n",
    "        agent_name: Name of the agent to use\n",
    "        model_override: Optional model configuration override\n",
    "        session_id: Session ID for tracking\n",
    "    \n",
    "    Returns:\n",
    "        Configured CascadeOrchestratorAdapter\n",
    "    \"\"\"\n",
    "    # Load all agents\n",
    "    all_agents = discover_agents()\n",
    "    \n",
    "    # Apply model override if provided\n",
    "    if model_override and agent_name in all_agents:\n",
    "        agent = all_agents[agent_name]\n",
    "        deployment_id = model_override.get('deployment_id', agent.model.deployment_id)\n",
    "        \n",
    "        # Detect model family from deployment_id\n",
    "        deployment_lower = deployment_id.lower()\n",
    "        \n",
    "        # Models that use max_completion_tokens instead of max_tokens\n",
    "        uses_max_completion_tokens = any(x in deployment_lower for x in [\n",
    "            'o1', 'o3', 'o4',           # Reasoning models\n",
    "            'gpt-5', 'gpt5',            # GPT-5 family\n",
    "            'gpt-4.1', 'gpt4.1',        # GPT-4.1 family\n",
    "        ])\n",
    "        \n",
    "        # Models that don't support custom temperature (only default=1)\n",
    "        # o-series: reasoning models\n",
    "        # gpt-5: only supports default temperature\n",
    "        no_custom_temperature = any(x in deployment_lower for x in [\n",
    "            'o1', 'o3', 'o4',           # Reasoning models\n",
    "            'gpt-5', 'gpt5',            # GPT-5 family (only default temp)\n",
    "        ])\n",
    "        \n",
    "        # For new-gen models: use max_completion_tokens, not max_tokens\n",
    "        if uses_max_completion_tokens:\n",
    "            max_tokens_val = None  # New models don't support max_tokens\n",
    "            max_completion_tokens_val = model_override.get(\n",
    "                'max_completion_tokens', \n",
    "                model_override.get('max_tokens', 4096)  # Allow fallback from max_tokens\n",
    "            )\n",
    "        else:\n",
    "            max_tokens_val = model_override.get('max_tokens', agent.model.max_tokens)\n",
    "            max_completion_tokens_val = model_override.get('max_completion_tokens')\n",
    "        \n",
    "        # Temperature handling\n",
    "        if no_custom_temperature:\n",
    "            temperature_val = None  # These models only support default (1)\n",
    "        else:\n",
    "            temperature_val = model_override.get('temperature', agent.model.temperature)\n",
    "        \n",
    "        # Reasoning effort only for o-series\n",
    "        is_reasoning_model = any(x in deployment_lower for x in ['o1', 'o3', 'o4'])\n",
    "        reasoning_effort = model_override.get('reasoning_effort', 'medium') if is_reasoning_model else model_override.get('reasoning_effort')\n",
    "        \n",
    "        # Create new model config with proper parameters for model type\n",
    "        model_config = ModelConfig(\n",
    "            deployment_id=deployment_id,\n",
    "            endpoint_preference=model_override.get('endpoint_preference', agent.model.endpoint_preference),\n",
    "            verbosity=model_override.get('verbosity', agent.model.verbosity),\n",
    "            temperature=temperature_val,\n",
    "            max_tokens=max_tokens_val,\n",
    "            max_completion_tokens=max_completion_tokens_val,\n",
    "            reasoning_effort=reasoning_effort,\n",
    "        )\n",
    "        \n",
    "        # Update agent's model config\n",
    "        agent.model = model_config\n",
    "        agent.cascade_model = model_config\n",
    "    \n",
    "    # Build handoff map\n",
    "    handoff_map = build_handoff_map(all_agents)\n",
    "    \n",
    "    # Create orchestrator\n",
    "    orchestrator = CascadeOrchestratorAdapter.create(\n",
    "        start_agent=agent_name,\n",
    "        session_id=session_id,\n",
    "        call_connection_id=f\"eval-{session_id}\",\n",
    "        agents=all_agents,\n",
    "        handoff_map=handoff_map,\n",
    "        enable_rag=False,  # Disable RAG for faster eval\n",
    "        streaming=False,   # Non-streaming for eval\n",
    "    )\n",
    "    \n",
    "    return orchestrator\n",
    "\n",
    "print(\"‚úÖ create_orchestrator() function defined\")\n",
    "print(\"   - o-series/gpt-5: max_completion_tokens, NO temperature\")\n",
    "print(\"   - gpt-4.1: max_completion_tokens, with temperature\")\n",
    "print(\"   - gpt-4o: max_tokens + temperature (legacy)\")\n",
    "print(\"\\nYou can now create orchestrators with:\")\n",
    "print(\"  orchestrator = create_orchestrator('FraudAgent')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run a Single Turn (Real Agent!)\n",
    "\n",
    "Test a real agent with a real query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_single_turn(\n",
    "    agent_name: str,\n",
    "    user_query: str,\n",
    "    model_override: dict = None,\n",
    "    record_events: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Run a single turn with the cascade orchestrator.\n",
    "    \n",
    "    Args:\n",
    "        agent_name: Agent to use\n",
    "        user_query: User's question/request\n",
    "        model_override: Optional model config override\n",
    "        record_events: Whether to record events\n",
    "    \n",
    "    Returns:\n",
    "        Result dictionary with response and metrics\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from tests.evaluation import MockMemoManager\n",
    "    \n",
    "    session_id = f\"eval-{int(time.time())}\"\n",
    "    \n",
    "    # Create MemoManager for session state\n",
    "    memo_manager = MockMemoManager(\n",
    "        session_id=session_id,\n",
    "        context={\"caller_name\": \"Test User\", \"client_id\": \"test_client\"}\n",
    "    )\n",
    "    \n",
    "    # Create orchestrator\n",
    "    orchestrator = create_orchestrator(\n",
    "        agent_name=agent_name,\n",
    "        model_override=model_override,\n",
    "        session_id=session_id,\n",
    "    )\n",
    "    \n",
    "    # DEBUG: Verify model config is correctly applied on the orchestrator's agent\n",
    "    if model_override:\n",
    "        # Get the agent from the orchestrator's internal state (not a fresh discover)\n",
    "        agent = orchestrator.agents.get(agent_name)\n",
    "        if agent:\n",
    "            actual_model = agent.get_model_for_mode(\"cascade\")\n",
    "            print(f\"  ‚û°Ô∏è  Model config applied: deployment={actual_model.deployment_id}, \"\n",
    "                  f\"verbosity={actual_model.verbosity}, \"\n",
    "                  f\"endpoint_pref={actual_model.endpoint_preference}\")\n",
    "    \n",
    "    # Optionally wrap with recorder\n",
    "    if record_events:\n",
    "        recorder = EventRecorder(\n",
    "            run_id=f\"{agent_name}_{session_id}\",\n",
    "            output_dir=PROJECT_ROOT / \"runs\" / \"jupyter_tests\",\n",
    "        )\n",
    "        orchestrator = EvaluationOrchestratorWrapper(\n",
    "            orchestrator=orchestrator,\n",
    "            recorder=recorder,\n",
    "        )\n",
    "    \n",
    "    # Create context WITH MemoManager!\n",
    "    context = OrchestratorContext(\n",
    "        session_id=session_id,\n",
    "        user_text=user_query,\n",
    "        turn_id=\"turn_1\",\n",
    "        conversation_history=memo_manager.get_history(agent_name),\n",
    "        metadata={\n",
    "            \"scenario\": \"jupyter_test\",\n",
    "            \"memo_manager\": memo_manager,  # ‚Üê THE KEY!\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # Run turn through orchestrator\n",
    "    start_time = time.time()\n",
    "    result = await orchestrator.process_turn(context)\n",
    "    elapsed_ms = (time.time() - start_time) * 1000\n",
    "    \n",
    "    # Update history\n",
    "    memo_manager.append_to_history(agent_name, \"user\", user_query)\n",
    "    memo_manager.append_to_history(agent_name, \"assistant\", result.response_text)\n",
    "    \n",
    "    return {\n",
    "        \"query\": user_query,\n",
    "        \"agent\": agent_name,\n",
    "        \"response\": result.response_text,\n",
    "        \"model\": model_override.get('deployment_id') if model_override else 'default',\n",
    "        \"endpoint\": model_override.get('endpoint_preference') if model_override else 'auto',\n",
    "        \"verbosity\": model_override.get('verbosity') if model_override else 'default',\n",
    "        \"input_tokens\": result.input_tokens,\n",
    "        \"output_tokens\": result.output_tokens,\n",
    "        \"latency_ms\": elapsed_ms,\n",
    "        \"error\": result.error,\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ run_single_turn() function defined\")\n",
    "print(\"   Evaluates agent with full orchestrator execution\")\n",
    "print(\"   Now includes verbosity debugging output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test It! Run a Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Evaluate FraudAgent with GPT-4o\n",
    "result = await run_single_turn(\n",
    "    agent_name=\"FraudAgent\",\n",
    "    user_query=\"I see a $500 charge from Amazon that I didn't make\",\n",
    "    model_override={\n",
    "        \"deployment_id\": \"gpt-4o\",\n",
    "        \"endpoint_preference\": \"responses\",  # Use responses API for verbosity support\n",
    "        \"verbosity\": 0,  # Minimal verbosity for faster responses\n",
    "        \"temperature\": 0.7,\n",
    "    },\n",
    "    record_events=True,\n",
    ")\n",
    "\n",
    "# Display result\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéØ EVALUATION RESULT\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nüìù Query: {result['query']}\")\n",
    "print(f\"\\nü§ñ Agent: {result['agent']}\")\n",
    "print(f\"üí¨ Response:\\n{result['response']}\")\n",
    "print(f\"\\nüìä Metrics:\")\n",
    "print(f\"  ‚Ä¢ Model: {result['model']}\")\n",
    "print(f\"  ‚Ä¢ Endpoint: {result['endpoint']}\")\n",
    "print(f\"  ‚Ä¢ Verbosity: {result['verbosity']}\")\n",
    "print(f\"  ‚Ä¢ Input tokens: {result['input_tokens']}\")\n",
    "print(f\"  ‚Ä¢ Output tokens: {result['output_tokens']}\")\n",
    "print(f\"  ‚Ä¢ Latency: {result['latency_ms']:.0f}ms\")\n",
    "if result['error']:\n",
    "    print(f\"  ‚Ä¢ ‚ùå Error: {result['error']}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Model Configurations (i.e gpt-5-mini vs gpt-4o from the above)\n",
    "\n",
    "Test the same query with different model configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def compare_model_configs(\n",
    "    agent_name: str,\n",
    "    user_query: str,\n",
    "    configs: list[dict],\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Run the same query with different model configurations.\n",
    "    \n",
    "    Args:\n",
    "        agent_name: Agent to test\n",
    "        user_query: Query to test\n",
    "        configs: List of model config dictionaries\n",
    "    \n",
    "    Returns:\n",
    "        List of results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for config in configs:\n",
    "        print(f\"Testing {config.get('deployment_id')} with {config.get('endpoint_preference')}...\")\n",
    "        \n",
    "        result = await run_single_turn(\n",
    "            agent_name=agent_name,\n",
    "            user_query=user_query,\n",
    "            model_override=config,\n",
    "            record_events=True,\n",
    "        )\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: Compare GPT-4o Chat vs Responses API\n",
    "comparison_results = await compare_model_configs(\n",
    "    agent_name=\"FraudAgent\",\n",
    "    user_query=\"I see a suspicious $500 charge\",\n",
    "    configs=[\n",
    "        {\n",
    "            \"deployment_id\": \"gpt-5-nano\",\n",
    "            \"endpoint_preference\": \"responses\",\n",
    "            \"verbosity\": 1,\n",
    "            \"temperature\": 0.7,\n",
    "        },\n",
    "        {\n",
    "            \"deployment_id\": \"gpt-5-nano\",\n",
    "            \"endpoint_preference\": \"responses\",\n",
    "            \"verbosity\": 0,  # Minimal\n",
    "        },\n",
    "        {\n",
    "            \"deployment_id\": \"gpt-5-nano\",\n",
    "            \"endpoint_preference\": \"responses\",\n",
    "            \"verbosity\": 2,  # Detailed\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Display comparison\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä MODEL CONFIGURATION COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "for i, result in enumerate(comparison_results, 1):\n",
    "    print(f\"\\n{i}. {result['model']} | {result['endpoint']}\")\n",
    "    print(f\"   Response length: {len(result['response'])} chars\")\n",
    "    print(f\"   Output tokens: {result['output_tokens']}\")\n",
    "    print(f\"   Latency: {result['latency_ms']:.0f}ms\")\n",
    "    print(f\"   Response: {result['response'][:150]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic: Verify Model Parameters\n",
    "\n",
    "Let's verify what parameters are actually being sent to the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic: Check what parameters are being prepared for the API call\n",
    "from src.aoai.manager import AzureOpenAIManager\n",
    "from apps.artagent.backend.registries.agentstore.base import ModelConfig\n",
    "\n",
    "def diagnose_model_params(model_override: dict):\n",
    "    \"\"\"\n",
    "    Diagnose what parameters would be sent to the API.\n",
    "    \n",
    "    This shows exactly what the AzureOpenAIManager prepares\n",
    "    based on your model_override settings.\n",
    "    \"\"\"\n",
    "    # Create a ModelConfig from the override\n",
    "    config = ModelConfig(\n",
    "        deployment_id=model_override.get('deployment_id', 'gpt-4o'),\n",
    "        endpoint_preference=model_override.get('endpoint_preference', 'auto'),\n",
    "        verbosity=model_override.get('verbosity', 0),\n",
    "        temperature=model_override.get('temperature'),\n",
    "        max_tokens=model_override.get('max_tokens'),\n",
    "        max_completion_tokens=model_override.get('max_completion_tokens'),\n",
    "        reasoning_effort=model_override.get('reasoning_effort'),\n",
    "    )\n",
    "    \n",
    "    print(f\"üìã ModelConfig created:\")\n",
    "    print(f\"   ‚Ä¢ deployment_id: {config.deployment_id}\")\n",
    "    print(f\"   ‚Ä¢ endpoint_preference: {config.endpoint_preference}\")\n",
    "    print(f\"   ‚Ä¢ verbosity: {config.verbosity}\")\n",
    "    print(f\"   ‚Ä¢ temperature: {config.temperature}\")\n",
    "    print(f\"   ‚Ä¢ max_tokens: {config.max_tokens}\")\n",
    "    print(f\"   ‚Ä¢ max_completion_tokens: {config.max_completion_tokens}\")\n",
    "    print(f\"   ‚Ä¢ reasoning_effort: {config.reasoning_effort}\")\n",
    "    \n",
    "    # Create a temporary manager to check parameter preparation\n",
    "    manager = AzureOpenAIManager(enable_tracing=False)\n",
    "    \n",
    "    # Check which endpoint would be used\n",
    "    use_responses = manager._should_use_responses_endpoint(config)\n",
    "    print(f\"\\nüîÄ Endpoint decision: {'RESPONSES' if use_responses else 'CHAT'}\")\n",
    "    \n",
    "    # Sample messages for testing\n",
    "    test_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello\"},\n",
    "    ]\n",
    "    \n",
    "    # Get prepared parameters\n",
    "    if use_responses:\n",
    "        params = manager._prepare_responses_params(config, test_messages)\n",
    "        print(f\"\\nüì¶ Responses API params:\")\n",
    "    else:\n",
    "        params = manager._prepare_chat_params(config, test_messages)\n",
    "        print(f\"\\nüì¶ Chat API params:\")\n",
    "    \n",
    "    # Show key parameters (excluding messages for brevity)\n",
    "    for key, value in params.items():\n",
    "        if key != 'messages' and key != 'input':\n",
    "            print(f\"   ‚Ä¢ {key}: {value}\")\n",
    "    \n",
    "    # Check if verbosity is included\n",
    "    if 'verbosity' in params:\n",
    "        print(f\"\\n‚úÖ verbosity IS being sent to API: {params['verbosity']}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  verbosity is NOT in the API params!\")\n",
    "    \n",
    "    return params\n",
    "\n",
    "# Test with different configurations\n",
    "print(\"=\" * 70)\n",
    "print(\"üîç DIAGNOSTIC: Testing verbosity parameter passing\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n--- Test 1: GPT-4o with Chat endpoint ---\")\n",
    "diagnose_model_params({\n",
    "    \"deployment_id\": \"gpt-4o\",\n",
    "    \"endpoint_preference\": \"chat\",\n",
    "    \"verbosity\": 0,\n",
    "})\n",
    "\n",
    "print(\"\\n--- Test 2: GPT-4o with Responses endpoint ---\")\n",
    "diagnose_model_params({\n",
    "    \"deployment_id\": \"gpt-4o\",\n",
    "    \"endpoint_preference\": \"responses\",\n",
    "    \"verbosity\": 0,\n",
    "})\n",
    "\n",
    "print(\"\\n--- Test 3: GPT-5-mini with Responses endpoint ---\")\n",
    "diagnose_model_params({\n",
    "    \"deployment_id\": \"gpt-5-mini\",\n",
    "    \"endpoint_preference\": \"responses\",\n",
    "    \"verbosity\": 0,\n",
    "    \"reasoning_effort\": \"low\",\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Score Recorded Events\n",
    "\n",
    "Load and score events that were recorded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find recorded events\n",
    "runs_dir = PROJECT_ROOT / \"runs\" / \"jupyter_tests\"\n",
    "\n",
    "if runs_dir.exists():\n",
    "    event_files = list(runs_dir.glob(\"*_events.jsonl\"))\n",
    "    \n",
    "    if event_files:\n",
    "        print(f\"üìÇ Found {len(event_files)} event file(s):\\n\")\n",
    "        for f in event_files:\n",
    "            print(f\"  ‚Ä¢ {f.name}\")\n",
    "        \n",
    "        # Score the most recent one\n",
    "        latest_events = event_files[-1]\n",
    "        print(f\"\\nüìä Scoring: {latest_events.name}\\n\")\n",
    "        \n",
    "        # Load and score\n",
    "        scorer = MetricsScorer()\n",
    "        events = scorer.load_events(latest_events)\n",
    "        \n",
    "        # Generate summary\n",
    "        summary = scorer.generate_summary(\n",
    "            events,\n",
    "            scenario_name=\"jupyter_test\",\n",
    "        )\n",
    "        \n",
    "        # Display summary\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Scenario: {summary.scenario_name}\")\n",
    "        print(f\"Agent: {summary.agent_name}\")\n",
    "        print(f\"Total Turns: {summary.total_turns}\")\n",
    "        print(f\"\\nüîß Tool Metrics:\")\n",
    "        print(f\"  Precision: {summary.tool_metrics['precision']:.2%}\")\n",
    "        print(f\"  Recall: {summary.tool_metrics['recall']:.2%}\")\n",
    "        print(f\"  Efficiency: {summary.tool_metrics['efficiency']:.2%}\")\n",
    "        print(f\"\\n‚è±Ô∏è  Latency:\")\n",
    "        print(f\"  P50: {summary.latency_metrics['e2e_p50_ms']:.1f}ms\")\n",
    "        print(f\"  P95: {summary.latency_metrics['e2e_p95_ms']:.1f}ms\")\n",
    "        print(f\"\\nüí∞ Cost:\")\n",
    "        total_tokens = summary.cost_analysis['total_input_tokens'] + summary.cost_analysis['total_output_tokens']\n",
    "        print(f\"  Total tokens: {total_tokens:,}\")\n",
    "        print(f\"  Estimated cost: ${summary.cost_analysis['estimated_cost_usd']:.4f}\")\n",
    "        print(\"=\" * 60)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No event files found. Run some tests first!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Runs directory not found. Run some tests first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interactive Widget for Testing\n",
    "\n",
    "A simple widget to test different agents and queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive widgets (fresh instances each run to avoid handler accumulation)\n",
    "agent_selector = widgets.Dropdown(\n",
    "    options=sorted(list(agents.keys())),\n",
    "    description='Agent:',\n",
    "    style={'description_width': '100px'},\n",
    ")\n",
    "\n",
    "model_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        # GPT-4 family\n",
    "        'gpt-4o',\n",
    "        'gpt-4o-mini',\n",
    "        'gpt-4.1',\n",
    "        'gpt-4.1-mini',\n",
    "        'gpt-4.1-nano',\n",
    "        # GPT-5 family\n",
    "        'gpt-5',\n",
    "        'gpt-5-chat',\n",
    "        'gpt-5-mini',\n",
    "        # o-series (reasoning models)\n",
    "        'o1',\n",
    "        'o1-mini',\n",
    "        'o1-preview',\n",
    "        'o3',\n",
    "        'o3-mini',\n",
    "        'o4-mini',\n",
    "        'gpt-oss-120b'\n",
    "    ],\n",
    "    value='gpt-4o',\n",
    "    description='Model:',\n",
    "    style={'description_width': '100px'},\n",
    ")\n",
    "\n",
    "endpoint_selector = widgets.Dropdown(\n",
    "    options=['auto', 'chat', 'responses'],\n",
    "    value='auto',\n",
    "    description='Endpoint:',\n",
    "    style={'description_width': '100px'},\n",
    ")\n",
    "\n",
    "query_input = widgets.Textarea(\n",
    "    value='I need help with my account',\n",
    "    description='Query:',\n",
    "    style={'description_width': '100px'},\n",
    "    layout=widgets.Layout(width='500px', height='80px')\n",
    ")\n",
    "\n",
    "# Create fresh button each run (avoids accumulating click handlers)\n",
    "test_button = widgets.Button(\n",
    "    description='Run Test',\n",
    "    button_style='success',\n",
    "    icon='play',\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Apply nest_asyncio once at module level\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def on_button_click(b):\n",
    "    \"\"\"Handle button click - run async test.\"\"\"\n",
    "    with output_area:\n",
    "        output_area.clear_output(wait=True)\n",
    "        print(\"‚è≥ Running test...\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Get the current event loop and run the coroutine\n",
    "            loop = asyncio.get_event_loop()\n",
    "            result = loop.run_until_complete(run_single_turn(\n",
    "                agent_name=agent_selector.value,\n",
    "                user_query=query_input.value,\n",
    "                model_override={\n",
    "                    \"deployment_id\": model_selector.value,\n",
    "                    \"endpoint_preference\": endpoint_selector.value,\n",
    "                },\n",
    "            ))\n",
    "            \n",
    "            print(\"‚úÖ Test Complete\\n\")\n",
    "            print(f\"Agent: {result['agent']}\")\n",
    "            print(f\"Model: {result['model']} ({result['endpoint']})\\n\")\n",
    "            print(f\"Response:\\n{result['response']}\\n\")\n",
    "            print(f\"Tokens: {result['output_tokens']} | Latency: {result['latency_ms']:.0f}ms\")\n",
    "            \n",
    "            if result['error']:\n",
    "                print(f\"\\n‚ö†Ô∏è Error: {result['error']}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "# Register handler on fresh button\n",
    "test_button.on_click(on_button_click)\n",
    "\n",
    "# Build and display widget UI\n",
    "widget_ui = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>üß™ Interactive Agent Tester</h3>\"),\n",
    "    agent_selector,\n",
    "    model_selector,\n",
    "    endpoint_selector,\n",
    "    query_input,\n",
    "    test_button,\n",
    "    output_area,\n",
    "])\n",
    "\n",
    "display(widget_ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load YAML Comparison\n",
    "\n",
    "Load the fraud detection comparison YAML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load comparison YAML\n",
    "comparison_path = PROJECT_ROOT / \"tests\" / \"eval_scenarios\" / \"ab_tests\" / \"fraud_detection_comparison.yaml\"\n",
    "\n",
    "if comparison_path.exists():\n",
    "    runner = ComparisonRunner(\n",
    "        comparison_path=comparison_path,\n",
    "        output_dir=PROJECT_ROOT / \"runs\" / \"jupyter_comparison\",\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Loaded: {runner.comparison['comparison_name']}\")\n",
    "    print(f\"\\nVariants:\")\n",
    "    for variant in runner.comparison['variants']:\n",
    "        print(f\"  ‚Ä¢ {variant['variant_id']}\")\n",
    "        model = variant.get('model_override', {})\n",
    "        print(f\"    Model: {model.get('deployment_id')}\")\n",
    "        print(f\"    Endpoint: {model.get('endpoint_preference')}\")\n",
    "    \n",
    "    print(f\"\\nNote: To run this comparison with real orchestrators,\")\n",
    "    print(f\"      you'd need to implement the full comparison runner.\")\n",
    "    print(f\"      For now, use the compare_model_configs() function above.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Comparison YAML not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "You now have:\n",
    "\n",
    "‚úÖ **Real agent testing** - Not mocks, actual orchestrator\n",
    "‚úÖ **Event recording** - All turns recorded to JSONL\n",
    "‚úÖ **Metrics scoring** - Performance analysis\n",
    "‚úÖ **Model comparison** - Test different configs\n",
    "‚úÖ **Real-time optimization** - Best practices for minimal latency\n",
    "\n",
    "### Try These:\n",
    "\n",
    "1. **Test different agents**:\n",
    "   ```python\n",
    "   result = await run_single_turn(\n",
    "       agent_name=\"InvestmentAdvisor\",\n",
    "       user_query=\"How's my 401k doing?\",\n",
    "   )\n",
    "   ```\n",
    "\n",
    "2. **Compare endpoints**:\n",
    "   ```python\n",
    "   results = await compare_model_configs(\n",
    "       agent_name=\"FraudAgent\",\n",
    "       user_query=\"Suspicious charge\",\n",
    "       configs=[{...}, {...}],\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **Analyze recorded events**:\n",
    "   - Events are saved to `runs/jupyter_tests/`\n",
    "   - Use MetricsScorer to analyze them\n",
    "\n",
    "4. **Optimize for real-time**:\n",
    "   - See the Real-Time Optimization section above\n",
    "   - Use `verbosity=0`, `reasoning_effort=\"low\"`, and capped tokens\n",
    "   - Test with the benchmark to measure improvements\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Evaluation Package](../../../apps/artagent/backend/evaluation/)\n",
    "- [Full Documentation](../../../docs/testing/model-evals.md)\n",
    "- [YAML Scenarios](../../../tests/eval_scenarios/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-time optimization comparison\n",
    "# Compare optimal vs suboptimal configurations\n",
    "\n",
    "import time\n",
    "\n",
    "async def benchmark_realtime_configs():\n",
    "    \"\"\"\n",
    "    Benchmark optimal vs suboptimal real-time configurations.\n",
    "    Shows the impact of proper configuration on latency.\n",
    "    \"\"\"\n",
    "    test_query = \"I see a suspicious charge on my account\"\n",
    "    \n",
    "    # ‚úÖ OPTIMAL Real-Time Configuration\n",
    "    optimal_config = {\n",
    "        \"deployment_id\": \"gpt-5-mini\",\n",
    "        \"endpoint_preference\": \"responses\",\n",
    "        \"verbosity\": 0,  # Minimal - fastest\n",
    "        \"reasoning_effort\": \"low\",  # Fast reasoning\n",
    "        \"max_completion_tokens\": 2048,  # Reasonable limit\n",
    "    }\n",
    "    \n",
    "    # ‚ö†Ô∏è SUBOPTIMAL Configuration (for comparison)\n",
    "    suboptimal_config = {\n",
    "        \"deployment_id\": \"gpt-5-mini\", \n",
    "        \"endpoint_preference\": \"responses\",\n",
    "        \"verbosity\": 2,  # Detailed - slower\n",
    "        # No reasoning_effort set - will use default \"medium\"\n",
    "        \"max_completion_tokens\": 8192,  # Excessive for real-time\n",
    "    }\n",
    "    \n",
    "    print(\"üöÄ Benchmarking Real-Time Configurations\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Test 1: Optimal configuration\n",
    "    print(\"\\n‚úÖ Testing OPTIMAL configuration (verbosity=0, low reasoning, capped tokens)...\")\n",
    "    start = time.time()\n",
    "    result_optimal = await run_single_turn(\n",
    "        agent_name=\"FraudAgent\",\n",
    "        user_query=test_query,\n",
    "        model_override=optimal_config,\n",
    "        record_events=False,  # Skip recording for faster benchmark\n",
    "    )\n",
    "    optimal_latency = (time.time() - start) * 1000\n",
    "    \n",
    "    # Test 2: Suboptimal configuration  \n",
    "    print(f\"\\n‚ö†Ô∏è  Testing SUBOPTIMAL configuration (verbosity=2, higher tokens)...\")\n",
    "    start = time.time()\n",
    "    result_suboptimal = await run_single_turn(\n",
    "        agent_name=\"FraudAgent\",\n",
    "        user_query=test_query,\n",
    "        model_override=suboptimal_config,\n",
    "        record_events=False,\n",
    "    )\n",
    "    suboptimal_latency = (time.time() - start) * 1000\n",
    "    \n",
    "    # Display comparison\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä REAL-TIME OPTIMIZATION RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\n‚úÖ OPTIMAL Configuration:\")\n",
    "    print(f\"   ‚Ä¢ Verbosity: 0 (minimal)\")\n",
    "    print(f\"   ‚Ä¢ Reasoning: low\")\n",
    "    print(f\"   ‚Ä¢ Max tokens: 2048\")\n",
    "    print(f\"   ‚Ä¢ Response length: {len(result_optimal['response'])} chars\")\n",
    "    print(f\"   ‚Ä¢ Output tokens: {result_optimal['output_tokens']}\")\n",
    "    print(f\"   ‚Ä¢ Latency: {optimal_latency:.0f}ms\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  SUBOPTIMAL Configuration:\")\n",
    "    print(f\"   ‚Ä¢ Verbosity: 2 (detailed)\")\n",
    "    print(f\"   ‚Ä¢ Reasoning: default\")\n",
    "    print(f\"   ‚Ä¢ Max tokens: 8192\")\n",
    "    print(f\"   ‚Ä¢ Response length: {len(result_suboptimal['response'])} chars\")\n",
    "    print(f\"   ‚Ä¢ Output tokens: {result_suboptimal['output_tokens']}\")\n",
    "    print(f\"   ‚Ä¢ Latency: {suboptimal_latency:.0f}ms\")\n",
    "    \n",
    "    # Calculate improvement\n",
    "    improvement_pct = ((suboptimal_latency - optimal_latency) / suboptimal_latency) * 100\n",
    "    latency_diff = suboptimal_latency - optimal_latency\n",
    "    \n",
    "    print(f\"\\nüéØ Performance Impact:\")\n",
    "    print(f\"   ‚Ä¢ Latency reduction: {latency_diff:.0f}ms ({improvement_pct:.1f}% faster)\")\n",
    "    print(f\"   ‚Ä¢ Token efficiency: {result_optimal['output_tokens']} vs {result_suboptimal['output_tokens']} tokens\")\n",
    "    \n",
    "    if improvement_pct > 10:\n",
    "        print(f\"\\n‚úÖ CONCLUSION: Optimal configuration provides {improvement_pct:.1f}% better latency!\")\n",
    "        print(f\"   For real-time voice, this translates to noticeably faster responses.\")\n",
    "    else:\n",
    "        print(f\"\\nüí° CONCLUSION: Both configs perform similarly (~{improvement_pct:.1f}% difference)\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return {\n",
    "        \"optimal\": result_optimal,\n",
    "        \"suboptimal\": result_suboptimal,\n",
    "        \"improvement_pct\": improvement_pct,\n",
    "    }\n",
    "\n",
    "# Run the benchmark\n",
    "print(\"‚è≥ Running real-time optimization benchmark...\\n\")\n",
    "benchmark_results = await benchmark_realtime_configs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "art-voice-agent-accelerator (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
