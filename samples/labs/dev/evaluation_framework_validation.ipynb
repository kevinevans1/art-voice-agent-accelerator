{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d53c9b2d",
   "metadata": {},
   "source": [
    "# Evaluation Framework Validation Notebook\n",
    "\n",
    "This notebook validates each step of the ART Voice Agent evaluation framework as outlined in the documentation.\n",
    "\n",
    "## What This Notebook Covers\n",
    "\n",
    "1. **Import & Environment Setup** - Verify all components are importable\n",
    "2. **Event Recording** - Test EventRecorder functionality\n",
    "3. **Event Loading & Inspection** - Load and analyze recorded events\n",
    "4. **Metrics Scoring** - Score individual turns and generate summaries\n",
    "5. **Scenario Execution** - Run YAML-based scenarios\n",
    "6. **A/B Comparison** - Compare model configurations\n",
    "7. **Azure AI Foundry Export** - Export to cloud evaluation format\n",
    "8. **CLI Validation** - Verify CLI commands work correctly\n",
    "\n",
    "> **Note**: This package should **never** be imported in production code. Import guards prevent usage when `ENV=production`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b51e2f",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries and Set Up Environment\n",
    "\n",
    "Import all evaluation framework components and verify the environment is correctly configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aa9022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up the project root for imports\n",
    "# From samples/labs/dev/ we need 3 levels up to reach project root\n",
    "PROJECT_ROOT = Path.cwd().parent.parent.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Verify we're not in production\n",
    "env = os.getenv(\"ENV\", \"development\")\n",
    "print(f\"‚úÖ Environment: {env}\")\n",
    "assert env != \"production\", \"Cannot run evaluation in production!\"\n",
    "\n",
    "print(f\"‚úÖ Project root: {PROJECT_ROOT}\")\n",
    "print(f\"‚úÖ Python path configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225c80ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment configuration (same pattern as the app)\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env files in order of precedence\n",
    "env_local_path = PROJECT_ROOT / \".env.local\"\n",
    "env_path = PROJECT_ROOT / \".env\"\n",
    "\n",
    "config_source = \"system environment\"\n",
    "if env_local_path.exists():\n",
    "    print(f\"‚úÖ Loading .env.local\")\n",
    "    load_dotenv(env_local_path, override=True)\n",
    "    config_source = \".env.local\"\n",
    "elif env_path.exists():\n",
    "    print(f\"‚úÖ Loading .env\")\n",
    "    load_dotenv(env_path, override=True)\n",
    "    config_source = \".env\"\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No .env file found. Using system environment variables.\")\n",
    "\n",
    "# Try to load Azure App Configuration (preferred)\n",
    "try:\n",
    "    from config.appconfig_provider import bootstrap_appconfig, get_provider_status\n",
    "    \n",
    "    appconfig_loaded = bootstrap_appconfig()\n",
    "    if appconfig_loaded:\n",
    "        status = get_provider_status()\n",
    "        endpoint_name = status.get(\"endpoint\", \"\").split(\"//\")[-1].split(\".\")[0] if status.get(\"endpoint\") else \"unknown\"\n",
    "        print(f\"‚úÖ Loaded configuration from Azure App Config ({endpoint_name})\")\n",
    "        config_source = f\"Azure App Config ({endpoint_name})\"\n",
    "except Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è  App Configuration not available, using {config_source}\")\n",
    "\n",
    "# Verify Azure OpenAI is configured\n",
    "endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "deployment = os.getenv('AZURE_OPENAI_CHAT_DEPLOYMENT_ID') or 'gpt-4o'\n",
    "\n",
    "print(f\"\\nüìã Configuration source: {config_source}\")\n",
    "if endpoint:\n",
    "    print(f\"‚úÖ Azure OpenAI endpoint: {endpoint}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  AZURE_OPENAI_ENDPOINT not set (some features may be limited)\")\n",
    "\n",
    "if deployment:\n",
    "    print(f\"‚úÖ Default deployment: {deployment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417e4a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation framework components\n",
    "from tests.evaluation import (\n",
    "    EventRecorder,\n",
    "    EvaluationOrchestratorWrapper,\n",
    "    MetricsScorer,\n",
    ")\n",
    "from tests.evaluation.schemas import (\n",
    "    TurnEvent,\n",
    "    ToolCall,\n",
    "    EvidenceBlob,\n",
    "    EvalModelConfig,\n",
    ")\n",
    "from tests.evaluation.foundry_exporter import FoundryExporter\n",
    "from tests.evaluation.mocks import MockMemoManager\n",
    "\n",
    "print(\"‚úÖ Core components imported:\")\n",
    "print(\"   - EventRecorder\")\n",
    "print(\"   - EvaluationOrchestratorWrapper\") \n",
    "print(\"   - MetricsScorer\")\n",
    "print(\"   - TurnEvent, ToolCall, EvidenceBlob schemas\")\n",
    "print(\"   - FoundryExporter\")\n",
    "print(\"   - MockMemoManager\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3aeba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports\n",
    "import json\n",
    "import subprocess\n",
    "import tempfile\n",
    "import shutil\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from uuid import uuid4\n",
    "\n",
    "def make_hash(data: dict | str) -> str:\n",
    "    \"\"\"Create SHA256 hash of data for ToolCall/EvidenceBlob.\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        data = json.dumps(data, sort_keys=True)\n",
    "    return hashlib.sha256(data.encode()).hexdigest()\n",
    "\n",
    "def make_excerpt(data: dict | str, max_len: int = 200) -> str:\n",
    "    \"\"\"Create excerpt of data (first 200 chars).\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        data = json.dumps(data)\n",
    "    return data[:max_len]\n",
    "\n",
    "# Create output directory for this notebook run\n",
    "NOTEBOOK_OUTPUT_DIR = PROJECT_ROOT / \"runs\" / \"notebook_validation\"\n",
    "NOTEBOOK_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Output directory: {NOTEBOOK_OUTPUT_DIR}\")\n",
    "print(f\"‚úÖ Helper functions: make_hash(), make_excerpt()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fb2fa6",
   "metadata": {},
   "source": [
    "## 2. Create EventRecorder and Record Events\n",
    "\n",
    "The `EventRecorder` captures orchestration events to JSONL format for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54261de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an EventRecorder for this validation run\n",
    "run_id = f\"notebook_validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "recorder = EventRecorder(\n",
    "    run_id=run_id,\n",
    "    output_dir=NOTEBOOK_OUTPUT_DIR\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ EventRecorder created\")\n",
    "print(f\"   Run ID: {run_id}\")\n",
    "print(f\"   Output file: {recorder.output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b761a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually record a sample conversation with multiple turns\n",
    "# This simulates what the EvaluationOrchestratorWrapper does automatically\n",
    "\n",
    "session_id = str(uuid4())\n",
    "base_ts = datetime.now()\n",
    "\n",
    "# --- Turn 1: User reports potential fraud ---\n",
    "turn1_id = \"turn_001\"\n",
    "turn1_start = base_ts.timestamp()\n",
    "\n",
    "recorder.record_turn_start(\n",
    "    turn_id=turn1_id,\n",
    "    agent=\"FraudDetectionAgent\",\n",
    "    user_text=\"I think someone stole my credit card. I see charges I didn't make.\",\n",
    "    timestamp=turn1_start\n",
    ")\n",
    "print(f\"‚úÖ Turn 1 started: {turn1_id}\")\n",
    "\n",
    "# Record tool call: check_recent_transactions\n",
    "tool1_start = turn1_start + 0.1\n",
    "recorder.record_tool_start(\n",
    "    tool_name=\"check_recent_transactions\",\n",
    "    arguments={\"account_id\": \"ACC-12345\", \"days\": 7},\n",
    "    timestamp=tool1_start\n",
    ")\n",
    "\n",
    "tool1_result = {\n",
    "    \"transactions\": [\n",
    "        {\"id\": \"TXN-001\", \"amount\": 499.99, \"merchant\": \"Unknown Store\", \"date\": \"2026-01-20\"},\n",
    "        {\"id\": \"TXN-002\", \"amount\": 150.00, \"merchant\": \"Gas Station\", \"date\": \"2026-01-19\"},\n",
    "    ]\n",
    "}\n",
    "tool1_end = tool1_start + 0.5\n",
    "recorder.record_tool_end(\n",
    "    tool_name=\"check_recent_transactions\",\n",
    "    result=tool1_result,\n",
    "    end_ts=tool1_end,\n",
    "    start_ts=tool1_start\n",
    ")\n",
    "print(f\"   ‚úÖ Tool call recorded: check_recent_transactions (500ms)\")\n",
    "\n",
    "# End turn 1 - tool_calls and evidence_blobs are built automatically from record_tool_* calls\n",
    "turn1_end = turn1_start + 1.2\n",
    "\n",
    "recorder.record_turn_end(\n",
    "    turn_id=turn1_id,\n",
    "    agent=\"FraudDetectionAgent\",\n",
    "    response_text=\"I found 2 recent transactions on your account. There's a charge of $499.99 at 'Unknown Store' on January 20th, and $150.00 at 'Gas Station' on January 19th. Can you confirm if either of these was unauthorized?\",\n",
    "    e2e_ms=1200,\n",
    "    timestamp=turn1_end,\n",
    "    ttft_ms=150,\n",
    "    model_config=EvalModelConfig(\n",
    "        model_name=\"gpt-4o\",\n",
    "        endpoint_used=\"chat\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=500\n",
    "    ),\n",
    "    input_tokens=250,\n",
    "    response_tokens=85\n",
    ")\n",
    "print(f\"‚úÖ Turn 1 completed (1200ms E2E)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b5188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Turn 2: User confirms fraud and requests action ---\n",
    "turn2_id = \"turn_002\"\n",
    "turn2_start = turn1_end + 2.0\n",
    "\n",
    "recorder.record_turn_start(\n",
    "    turn_id=turn2_id,\n",
    "    agent=\"FraudDetectionAgent\",\n",
    "    user_text=\"Yes, the $499.99 charge at Unknown Store is fraudulent. I didn't make that purchase.\",\n",
    "    timestamp=turn2_start\n",
    ")\n",
    "print(f\"‚úÖ Turn 2 started: {turn2_id}\")\n",
    "\n",
    "# Record tool calls: flag_transaction and create_dispute\n",
    "tool2a_start = turn2_start + 0.1\n",
    "tool2a_end = tool2a_start + 0.3\n",
    "tool2a_result = {\"status\": \"flagged\", \"case_id\": \"CASE-789\"}\n",
    "\n",
    "recorder.record_tool_start(\n",
    "    tool_name=\"flag_fraudulent_transaction\",\n",
    "    arguments={\"transaction_id\": \"TXN-001\", \"reason\": \"unauthorized\"},\n",
    "    timestamp=tool2a_start\n",
    ")\n",
    "recorder.record_tool_end(\n",
    "    tool_name=\"flag_fraudulent_transaction\",\n",
    "    result=tool2a_result,\n",
    "    end_ts=tool2a_end,\n",
    "    start_ts=tool2a_start\n",
    ")\n",
    "print(f\"   ‚úÖ Tool call recorded: flag_fraudulent_transaction (300ms)\")\n",
    "\n",
    "tool2b_start = tool2a_start + 0.4\n",
    "tool2b_end = tool2b_start + 0.4\n",
    "tool2b_result = {\"dispute_id\": \"DISP-456\", \"status\": \"pending\", \"estimated_resolution\": \"5-7 business days\"}\n",
    "\n",
    "recorder.record_tool_start(\n",
    "    tool_name=\"create_dispute\",\n",
    "    arguments={\"transaction_id\": \"TXN-001\", \"case_id\": \"CASE-789\"},\n",
    "    timestamp=tool2b_start\n",
    ")\n",
    "recorder.record_tool_end(\n",
    "    tool_name=\"create_dispute\",\n",
    "    result=tool2b_result,\n",
    "    end_ts=tool2b_end,\n",
    "    start_ts=tool2b_start\n",
    ")\n",
    "print(f\"   ‚úÖ Tool call recorded: create_dispute (400ms)\")\n",
    "\n",
    "# End turn 2 - tool_calls and evidence_blobs are built automatically\n",
    "turn2_end = turn2_start + 1.5\n",
    "\n",
    "recorder.record_turn_end(\n",
    "    turn_id=turn2_id,\n",
    "    agent=\"FraudDetectionAgent\",\n",
    "    response_text=\"I've flagged the $499.99 transaction as fraudulent and created dispute DISP-456. Your case ID is CASE-789. The dispute should be resolved within 5-7 business days. A provisional credit will be applied to your account within 24 hours.\",\n",
    "    e2e_ms=1500,\n",
    "    timestamp=turn2_end,\n",
    "    ttft_ms=180,\n",
    "    model_config=EvalModelConfig(\n",
    "        model_name=\"gpt-4o\",\n",
    "        endpoint_used=\"chat\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=500\n",
    "    ),\n",
    "    input_tokens=320,\n",
    "    response_tokens=95\n",
    ")\n",
    "print(f\"‚úÖ Turn 2 completed (1500ms E2E)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f25a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Turn 3: Handoff to specialist ---\n",
    "turn3_id = \"turn_003\"\n",
    "turn3_start = turn2_end + 1.5\n",
    "\n",
    "recorder.record_turn_start(\n",
    "    turn_id=turn3_id,\n",
    "    agent=\"FraudDetectionAgent\",\n",
    "    user_text=\"I want to speak with a supervisor about this.\",\n",
    "    timestamp=turn3_start\n",
    ")\n",
    "print(f\"‚úÖ Turn 3 started: {turn3_id}\")\n",
    "\n",
    "# Record handoff\n",
    "recorder.record_handoff(\n",
    "    source_agent=\"FraudDetectionAgent\",\n",
    "    target_agent=\"FraudSpecialistAgent\",\n",
    "    timestamp=turn3_start + 0.2\n",
    ")\n",
    "print(f\"   ‚úÖ Handoff recorded: FraudDetectionAgent ‚Üí FraudSpecialistAgent\")\n",
    "\n",
    "# End turn 3\n",
    "turn3_end = turn3_start + 0.8\n",
    "recorder.record_turn_end(\n",
    "    turn_id=turn3_id,\n",
    "    agent=\"FraudSpecialistAgent\",\n",
    "    response_text=\"I'm transferring you to our Fraud Specialist team. They will have full access to your case CASE-789 and can provide additional assistance. Please hold for a moment.\",\n",
    "    e2e_ms=800,\n",
    "    timestamp=turn3_end,\n",
    "    ttft_ms=120,\n",
    "    model_config=EvalModelConfig(\n",
    "        model_name=\"gpt-4o\",\n",
    "        endpoint_used=\"chat\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=500\n",
    "    ),\n",
    "    input_tokens=180,\n",
    "    response_tokens=55\n",
    ")\n",
    "print(f\"‚úÖ Turn 3 completed with handoff (800ms E2E)\")\n",
    "\n",
    "# Access the recorded events file path\n",
    "print(f\"\\n‚úÖ All events written to: {recorder.output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f46cfec",
   "metadata": {},
   "source": [
    "## 3. Load and Inspect Recorded Events\n",
    "\n",
    "Load the events from the JSONL file and inspect their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d40b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MetricsScorer to load events\n",
    "scorer = MetricsScorer()\n",
    "events = scorer.load_events(recorder.output_path)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(events)} events from JSONL file\")\n",
    "print(f\"\\nEvent types:\")\n",
    "for i, event in enumerate(events, 1):\n",
    "    tool_count = len(event.tool_calls) if event.tool_calls else 0\n",
    "    handoff = f\" ‚Üí {event.handoff.target_agent}\" if event.handoff else \"\"\n",
    "    print(f\"   Turn {i}: {event.agent_name} ({tool_count} tools){handoff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eaaa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect first event in detail\n",
    "event = events[0]\n",
    "print(\"üìã First Event Details:\")\n",
    "print(f\"   Turn ID:        {event.turn_id}\")\n",
    "print(f\"   Agent:          {event.agent_name}\")\n",
    "print(f\"   User Text:      {event.user_text[:60]}...\")\n",
    "print(f\"   Response:       {event.response_text[:60]}...\")\n",
    "print(f\"   E2E Latency:    {event.e2e_ms}ms\")\n",
    "print(f\"   TTFT:           {event.ttft_ms}ms\")\n",
    "print(f\"   Input Tokens:   {event.input_tokens}\")\n",
    "print(f\"   Response Tokens:{event.response_tokens}\")\n",
    "\n",
    "if event.tool_calls:\n",
    "    print(f\"\\nüîß Tool Calls:\")\n",
    "    for tc in event.tool_calls:\n",
    "        print(f\"   - {tc.name}: {tc.duration_ms}ms (status={tc.status})\")\n",
    "\n",
    "if event.evidence_blobs:\n",
    "    print(f\"\\nüìÑ Evidence Blobs:\")\n",
    "    for eb in event.evidence_blobs:\n",
    "        print(f\"   - [{eb.source}]: {eb.content_excerpt[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dc35f0",
   "metadata": {},
   "source": [
    "## 4. Score Individual Turns\n",
    "\n",
    "Use `MetricsScorer.score_turn()` to compute metrics for each turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80cbf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score each turn individually\n",
    "print(\"üìä Per-Turn Scores:\\n\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for event in events:\n",
    "    score = scorer.score_turn(event)\n",
    "    \n",
    "    print(f\"Turn {event.turn_id}:\")\n",
    "    print(f\"  üîß Tool Precision: {score.tool_precision:.2%}\")\n",
    "    print(f\"  üîß Tool Recall:    {score.tool_recall:.2%}\")\n",
    "    print(f\"  üîß Tool Efficiency:{score.tool_efficiency:.2%}\")\n",
    "    print(f\"  ‚úì  Grounded Ratio: {score.grounded_span_ratio:.2%}\")\n",
    "    print(f\"  ‚è±Ô∏è  E2E Latency:    {score.e2e_ms:.1f}ms\")\n",
    "    print(f\"  üìù Verbosity Score:{score.verbosity_score:.2f}\")\n",
    "    print(f\"  üìù Verbosity Tokens:{score.verbosity_tokens} / {score.verbosity_budget}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4798e5b6",
   "metadata": {},
   "source": [
    "## 5. Generate Summary Metrics\n",
    "\n",
    "Aggregate metrics across all turns using `MetricsScorer.generate_summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a94514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary across all events\n",
    "summary = scorer.generate_summary(events, scenario_name=\"fraud_detection_validation\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üìä EVALUATION SUMMARY: fraud_detection_validation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüîß Tool Metrics:\")\n",
    "print(f\"   Total Calls:  {summary.tool_metrics.get('total_calls', 0)}\")\n",
    "print(f\"   Precision:    {summary.tool_metrics.get('precision', 0):.2%}\")\n",
    "print(f\"   Recall:       {summary.tool_metrics.get('recall', 0):.2%}\")\n",
    "print(f\"   Efficiency:   {summary.tool_metrics.get('efficiency', 0):.2%}\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Latency Metrics:\")\n",
    "print(f\"   E2E P50:      {summary.latency_metrics.get('e2e_p50_ms', 0):.1f}ms\")\n",
    "print(f\"   E2E P95:      {summary.latency_metrics.get('e2e_p95_ms', 0):.1f}ms\")\n",
    "print(f\"   TTFT P50:     {summary.latency_metrics.get('ttft_p50_ms', 0):.1f}ms\")\n",
    "\n",
    "print(f\"\\n‚úì Groundedness Metrics:\")\n",
    "print(f\"   Grounded Ratio:     {summary.groundedness_metrics.get('avg_grounded_ratio', 0):.2%}\")\n",
    "print(f\"   Unsupported Claims: {summary.groundedness_metrics.get('avg_unsupported_claims', 0):.1f}\")\n",
    "\n",
    "print(f\"\\nüìù Verbosity Metrics:\")\n",
    "print(f\"   Avg Response Tokens: {summary.verbosity_metrics.get('avg_response_tokens', 0):.0f}\")\n",
    "print(f\"   Budget Violations:   {summary.verbosity_metrics.get('budget_violations', 0)}\")\n",
    "\n",
    "print(f\"\\nüí∞ Cost Analysis:\")\n",
    "print(f\"   Total Input Tokens:  {summary.cost_analysis.get('total_input_tokens', 0)}\")\n",
    "print(f\"   Total Output Tokens: {summary.cost_analysis.get('total_output_tokens', 0)}\")\n",
    "print(f\"   Estimated Cost:      ${summary.cost_analysis.get('estimated_cost_usd', 0):.4f}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84bc0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary to JSON file\n",
    "summary_file = NOTEBOOK_OUTPUT_DIR / \"summary.json\"\n",
    "with open(summary_file, \"w\") as f:\n",
    "    f.write(summary.model_dump_json(indent=2))\n",
    "\n",
    "print(f\"‚úÖ Summary saved to: {summary_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a2fb72",
   "metadata": {},
   "source": [
    "## 6. Export to Azure AI Foundry Format\n",
    "\n",
    "Use `FoundryExporter` to convert events to Azure AI Foundry compatible format for cloud-based evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6415a0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export events to Foundry format\n",
    "exporter = FoundryExporter()\n",
    "foundry_output = NOTEBOOK_OUTPUT_DIR / \"foundry_eval.jsonl\"\n",
    "\n",
    "# export_events takes a list of TurnEvents, not a path\n",
    "exporter.export_events(\n",
    "    events=events,\n",
    "    output_path=foundry_output,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Exported to Foundry format: {foundry_output}\")\n",
    "\n",
    "# Inspect the exported data\n",
    "print(f\"\\nüìã Foundry Dataset Sample:\")\n",
    "with open(foundry_output, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 2:  # Show first 2 records\n",
    "            break\n",
    "        record = json.loads(line)\n",
    "        print(f\"\\nRecord {i+1}:\")\n",
    "        print(f\"   Query:    {record.get('query', '')[:50]}...\")\n",
    "        print(f\"   Response: {record.get('response', '')[:50]}...\")\n",
    "        if record.get('context'):\n",
    "            print(f\"   Context:  {record.get('context', '')[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5649bd90",
   "metadata": {},
   "source": [
    "## 7. Validate CLI Commands\n",
    "\n",
    "Test the CLI commands to ensure they work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087f6f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CLI --help command\n",
    "result = subprocess.run(\n",
    "    [\"python\", \"-m\", \"tests.evaluation.cli\", \"--help\"],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    cwd=str(PROJECT_ROOT)\n",
    ")\n",
    "\n",
    "print(\"üìã CLI Help Output:\")\n",
    "print(\"-\" * 70)\n",
    "print(result.stdout)\n",
    "print(\"-\" * 70)\n",
    "print(f\"‚úÖ Exit code: {result.returncode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9f251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CLI score command with our recorded events\n",
    "cli_output_dir = NOTEBOOK_OUTPUT_DIR / \"cli_output\"\n",
    "cli_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\n",
    "        \"python\", \"-m\", \"tests.evaluation.cli\", \"score\",\n",
    "        \"--input\", str(recorder.output_path),\n",
    "        \"--output\", str(cli_output_dir)\n",
    "    ],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    cwd=str(PROJECT_ROOT)\n",
    ")\n",
    "\n",
    "print(\"üìã CLI Score Command Output:\")\n",
    "print(\"-\" * 70)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr[:500])\n",
    "print(\"-\" * 70)\n",
    "print(f\"‚úÖ Exit code: {result.returncode}\")\n",
    "\n",
    "# Check output files\n",
    "scores_file = cli_output_dir / \"scores.jsonl\"\n",
    "summary_file = cli_output_dir / \"summary.json\"\n",
    "\n",
    "print(f\"\\nüìÅ Output Files:\")\n",
    "print(f\"   scores.jsonl exists: {scores_file.exists()}\")\n",
    "print(f\"   summary.json exists: {summary_file.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb4605d",
   "metadata": {},
   "source": [
    "## 8. Score Existing Events (Real Data)\n",
    "\n",
    "If there are existing evaluation runs in the `runs/` directory, score them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3986a760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find existing events files in runs directory\n",
    "runs_dir = PROJECT_ROOT / \"runs\"\n",
    "events_files = list(runs_dir.glob(\"**/*events.jsonl\"))\n",
    "\n",
    "print(f\"üìÅ Found {len(events_files)} events files in runs/\")\n",
    "for f in events_files[:5]:  # Show first 5\n",
    "    relative_path = f.relative_to(runs_dir)\n",
    "    size_kb = f.stat().st_size / 1024\n",
    "    print(f\"   - {relative_path} ({size_kb:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8369afe",
   "metadata": {},
   "source": [
    "## 9. Submit to Azure AI Foundry\n",
    "\n",
    "Submit evaluation data to Azure AI Foundry for cloud-based evaluation with built-in evaluators (relevance, coherence, groundedness, safety, etc.).\n",
    "\n",
    "**Prerequisites:**\n",
    "- Azure AI Foundry project with endpoint URL\n",
    "- `AZURE_AI_FOUNDRY_PROJECT_ENDPOINT` set in `.env.local` or passed directly\n",
    "- Storage account connected to Foundry project (for studio_url generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be35fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have foundry_eval.jsonl files from our test data\n",
    "foundry_files = list(NOTEBOOK_OUTPUT_DIR.rglob(\"foundry_eval.jsonl\"))\n",
    "print(f\"üìÅ Found {len(foundry_files)} foundry_eval.jsonl files\")\n",
    "\n",
    "for f in foundry_files:\n",
    "    relative_path = f.relative_to(NOTEBOOK_OUTPUT_DIR)\n",
    "    size_kb = f.stat().st_size / 1024\n",
    "    print(f\"   - {relative_path} ({size_kb:.1f} KB)\")\n",
    "\n",
    "# Also check runs directory for any existing Foundry exports\n",
    "runs_foundry_files = list(runs_dir.rglob(\"foundry_eval.jsonl\")) if runs_dir.exists() else []\n",
    "print(f\"\\nüìÅ Found {len(runs_foundry_files)} foundry_eval.jsonl files in runs/\")\n",
    "for f in runs_foundry_files[:5]:\n",
    "    relative_path = f.relative_to(runs_dir)\n",
    "    size_kb = f.stat().st_size / 1024\n",
    "    print(f\"   - {relative_path} ({size_kb:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f0f258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export our test events to Foundry format first (if not already exported)\n",
    "from tests.evaluation.foundry_exporter import FoundryExporter\n",
    "from tests.evaluation.schemas import FoundryExportConfig\n",
    "\n",
    "# Create Foundry export from our test data\n",
    "foundry_export_path = NOTEBOOK_OUTPUT_DIR / \"foundry_eval.jsonl\"\n",
    "\n",
    "if events and not foundry_export_path.exists():\n",
    "    # Configure export with common evaluators\n",
    "    export_config = FoundryExportConfig(\n",
    "        enabled=True,\n",
    "        evaluators=[\"relevance\", \"coherence\", \"groundedness\", \"fluency\"],\n",
    "        include_metadata=True,\n",
    "        context_source=\"evidence\",\n",
    "    )\n",
    "    \n",
    "    exporter = FoundryExporter(export_config)\n",
    "    \n",
    "    # Export events\n",
    "    exporter.export_events(events, foundry_export_path)\n",
    "    print(f\"‚úÖ Exported {len(events)} events to {foundry_export_path}\")\n",
    "    \n",
    "    # Also generate evaluator config\n",
    "    evaluator_config_path = NOTEBOOK_OUTPUT_DIR / \"foundry_evaluators.json\"\n",
    "    exporter.generate_evaluator_config(evaluator_config_path)\n",
    "    print(f\"‚úÖ Generated evaluator config: {evaluator_config_path}\")\n",
    "else:\n",
    "    if foundry_export_path.exists():\n",
    "        print(f\"‚ÑπÔ∏è  Foundry export already exists: {foundry_export_path}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No events to export. Run the test data generation cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b894b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit to Azure AI Foundry\n",
    "# NOTE: Requires AZURE_AI_FOUNDRY_PROJECT_ENDPOINT to be set\n",
    "\n",
    "from tests.evaluation.foundry_exporter import submit_to_foundry\n",
    "import os\n",
    "\n",
    "# Get endpoint from environment (loaded from .env.local)\n",
    "foundry_endpoint = os.environ.get(\"AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\", \"\")\n",
    "\n",
    "if foundry_endpoint:\n",
    "    print(f\"‚úÖ Foundry endpoint configured: {foundry_endpoint[:50]}...\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  AZURE_AI_FOUNDRY_PROJECT_ENDPOINT not set\")\n",
    "    print(\"   Set it in .env.local or App Config to enable Foundry submission\")\n",
    "    print(\"   Format: https://<resource>.services.ai.azure.com/api/projects/<project>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5892432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Foundry evaluation (only if endpoint is configured)\n",
    "# This will:\n",
    "#   1. Run evaluators locally (relevance, coherence, groundedness, etc.)\n",
    "#   2. Log results to Azure AI Foundry portal\n",
    "#   3. Return a studio_url to view results in the portal\n",
    "\n",
    "if foundry_endpoint and foundry_export_path.exists():\n",
    "    print(\"üöÄ Submitting to Azure AI Foundry...\")\n",
    "    print(f\"   Data: {foundry_export_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Submit evaluation\n",
    "        foundry_result = await submit_to_foundry(\n",
    "            data_path=foundry_export_path,\n",
    "            evaluators_config_path=NOTEBOOK_OUTPUT_DIR / \"foundry_evaluators.json\",\n",
    "            project_endpoint=foundry_endpoint,\n",
    "            evaluation_name=f\"notebook_validation_{run_id}\",\n",
    "            model_deployment_name=\"gpt-4o\",  # For AI-based evaluators\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úÖ Foundry evaluation complete!\")\n",
    "        print(f\"   Evaluation: {foundry_result.get('evaluation_name')}\")\n",
    "        print(f\"   Status: {foundry_result.get('status')}\")\n",
    "        print(f\"   Rows: {foundry_result.get('rows_evaluated')}\")\n",
    "        \n",
    "        # Show metrics\n",
    "        metrics = foundry_result.get(\"metrics\", {})\n",
    "        if metrics:\n",
    "            print(\"\\nüìä Evaluation Metrics:\")\n",
    "            for name, value in metrics.items():\n",
    "                if isinstance(value, float):\n",
    "                    print(f\"   {name}: {value:.3f}\")\n",
    "                else:\n",
    "                    print(f\"   {name}: {value}\")\n",
    "        \n",
    "        # Show studio URL (most important!)\n",
    "        studio_url = foundry_result.get(\"studio_url\")\n",
    "        if studio_url:\n",
    "            print(f\"\\nüîó View in Azure AI Foundry:\")\n",
    "            print(f\"   {studio_url}\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  No studio_url returned - check that storage account is connected to Foundry project\")\n",
    "            \n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Missing dependency: {e}\")\n",
    "        print(\"   Install with: pip install azure-ai-evaluation\")\n",
    "    except ValueError as e:\n",
    "        print(f\"‚ùå Configuration error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Foundry submission failed: {e}\")\n",
    "else:\n",
    "    if not foundry_endpoint:\n",
    "        print(\"‚è≠Ô∏è  Skipping Foundry submission (endpoint not configured)\")\n",
    "    else:\n",
    "        print(f\"‚è≠Ô∏è  Skipping Foundry submission (no data at {foundry_export_path})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1bf7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Use CLI to submit to Foundry\n",
    "# This is useful for batch submissions or CI/CD pipelines\n",
    "\n",
    "print(\"üìã CLI Submit Command:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "if foundry_export_path.exists():\n",
    "    cli_cmd = f\"\"\"python -m tests.evaluation.cli submit \\\\\n",
    "    --input {foundry_export_path} \\\\\n",
    "    --endpoint \"$AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\" \\\\\n",
    "    --name \"notebook_validation\" \\\\\n",
    "    --model \"gpt-4o\" \"\"\"\n",
    "    print(cli_cmd)\n",
    "else:\n",
    "    print(\"# First generate foundry_eval.jsonl, then run:\")\n",
    "    print(\"\"\"python -m tests.evaluation.cli submit \\\\\n",
    "    --input path/to/foundry_eval.jsonl \\\\\n",
    "    --endpoint \"$AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\" \\\\\n",
    "    --name \"my_evaluation\" \\\\\n",
    "    --model \"gpt-4o\" \"\"\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(\"\\nüí° Tip: Run 'python -m tests.evaluation.cli submit --help' for all options\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "art-voice-agent-accelerator (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
